{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lMFghtLF_sOG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is a parameter?\n",
        "\n",
        "Answer :   \n",
        "\n",
        "Parameters are the essence of the model—once training is complete, the final set of parameter values defines the trained model. These values determine how new, unseen inputs will be transformed into outputs or predictions.\n",
        "\n",
        "1. A Parameter is a learnable value inside a model that gets updated during training to minimize prediction error.\n",
        "\n",
        "2. Parameters represent the model's knowledge learned from data.\n",
        "\n",
        "3. Parameters decide how the model converts input data into output predictions.\n",
        "\n",
        "4. They are automatically optimized using algorithms like gradient descent.\n",
        "\n",
        "5. Parameters are not set manually—they are found by fitting/training the model on data.\n",
        "\n",
        "6.  hyperparameters are set before training (like learning rate or number of layers) and control how training happens.\n",
        "\n",
        "6. Example :     \n",
        "\n",
        "- Coefficients in linear regression,\n",
        "\n",
        "- Weights and biases in neural networks\n",
        "\n",
        "- Centroids in K-means clustering\n"
      ],
      "metadata": {
        "id": "1Ln0lxcT_82D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. What is correlation?\n",
        "##What does negative correlation mean?\n",
        "\n",
        "\n",
        "Answer :  \n",
        "\n",
        "* Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "* Correlation tells how two variables move together — when one changes, does the other change too?\n",
        "* A correlation coefficient ranges from -1 to +1:\n",
        "\n",
        "    1. +1 means perfect positive correlation,\n",
        "    2. -1 means perfect negative correlation,\n",
        "    3. 0 means no correlation\n",
        "\n",
        "* Correlation only shows association, not cause-and-effect. Two variables can be correlated without one causing the other\n",
        "\n",
        "* It helps in feature selection and understanding variable relationships in machine learning datasets.\n",
        "\n",
        "** What does negative correlation mean?**\n",
        "\n",
        "Answer :   \n",
        "\n",
        "Negative correlation means two variables move in opposite directions. When one variable increases, the other decreases, and vice versa. It is also called inverse correlation.\n",
        "\n",
        "- -1 means erfect negative correlation: the variables have a perfectly inverse linear relationship.\n",
        "\n",
        "- value close to 0 means weak negative correlation.\n",
        "\n",
        "- Negative correlation is important in fields like finance for portfolio diversification where some assets move inversely to reduce risk.\n",
        "\n",
        "- Example : as temperature goes down, sweaters' sales tend to go up\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ztikPY3AAED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Answer :               \n",
        "\n",
        "Machine Learning is a branch of AI that allows computers to learn patterns from data and make predictions or decisions without being explicitly programmed. It automates learning from experience and use it in future.\n",
        "\n",
        "**components:**\n",
        "\n",
        "1. Data: Raw information used to train the model.\n",
        "2. Algorithm: The set of rules or models that learn from data (e.g., decision trees, neural networks).\n",
        "3. Training: The process where the algorithm learns from data.\n",
        "4. Model: The trained system that can make predictions.\n",
        "5. Evaluation: Testing how well the model works on new data.\n",
        "6. Hyperparameters: Settings used to control the training process (like learning rate).\n",
        "7. Prediction: Using the trained model to make decisions or forecasts on new data.\n"
      ],
      "metadata": {
        "id": "9vvx6egnASgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Answer :\n",
        "\n",
        "The loss value measures the difference between the model's predictions and the actual values. It quantifies how wrong the model is—lower loss means better predictions.\n",
        "\n",
        "During training, the model tries to minimize the loss by adjusting its internal parameters. If the loss decreases steadily over time, it means the model is learning well.\n",
        "\n",
        "A consistently high or fluctuating loss might mean the model is not learning properly.decreases on training data but not on unseen validation data, it may indicate overfitting — the model memorizes training data but fails to generalize and but don't decreases on training as well as test data means model underfit- model to simplistic doesn't capture pattern of the data very well.\n",
        "\n",
        "loss value is guiding to us  model overfit then reduce number features[model complexity] and model underfit then add more features/data, increase complexity, train on more epoch so we can improve ouer model accuracy."
      ],
      "metadata": {
        "id": "hXGdv84vAcQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. What are continuous and categorical variables?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Continuous variables:**  are numeric and can take any value within a range, including decimals.\n",
        "\n",
        "example : height, weight, or temperature\n",
        "\n",
        "These variables are typically used in regression problems.\n",
        "\n",
        "**Categorical variables:**  represent distinct categories or groups.\n",
        "\n",
        "types :    \n",
        "\n",
        "1. nomial : no order [ example : color: red, blue]\n",
        "2. ordinal : here is order [small, medium, large]\n",
        "\n",
        "Example : Gender (male, female), product type (A, B, C), or classes in classification tasks.\n",
        "\n",
        " categorical variables are often encoded to be used in models."
      ],
      "metadata": {
        "id": "9apYcMlrAiQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Answer :\n",
        "\n",
        "Categorical variables are non-numeric values like color or city that need to be converted into numbers before using them in models.\n",
        "\n",
        "**. Label Encoding :**\n",
        "\n",
        "* Assigns each category a unique integer label.\n",
        "* best for oridinal data , not for nomial data.\n",
        "* is not good for when cloumn have many unique value\n",
        "* example :  apple,banana,orange ---> 0,1,2\n",
        "\n",
        "**2.  One-Hot Encoding:**  \n",
        "*  Converts categorical variables into a binary matrix where each category is represented by a separate column.\n",
        "* best for nomial data.\n",
        "* example :  red,green, blue\n",
        "Red(1,0,0), Blue(0,1,0), Green(0,0,1)\n",
        "\n",
        "Disadvantage:==>\n",
        "* Increases dimensionality for high-cardinality features.\n",
        "* Avoid for high-cardinality features\n",
        "* can lead to sparse data and higher memory usage.\n",
        "\n",
        "**3. Ordinal Encoding :**\n",
        "* Converts categorical variables into integers based on their order or rank.\n",
        "*  Ideal for ordinal data—categories with a meaningful sequence (e.g., \"Low\", \"Medium\", \"High\").\n",
        "* Each category is mapped to a unique integer.\n",
        "\n",
        " Example: \"Low\" → 0, \"Medium\" → 1, \"High\" → 2\n",
        "\n",
        "*  Preserves order information\n",
        "\n",
        "**4. Target Encoding :**\n",
        "* Replaces each category with the mean of the target variable for that category.\n",
        "* useful for high-cardinality categorical features.\n",
        "* example : -  If category A has target values [1, 0, 1], then A → 0.67\n",
        "\n",
        "\n",
        "Advantage:==>\n",
        "1. Captures relationship between feature and target.\n",
        "2. Reduces dimensionality compared to one-hot encoding.\n",
        "\n",
        "Disadvantage:==>\n",
        "1. Can cause data leakage if not cross-validated/K-fold mean encoding\n",
        " properly.\n",
        "2. Apply smoothing to stabilize values for small category groups.\n",
        "\n",
        "**5. Frequency (Count) Encoding :**\n",
        "* Replaces each category with the number of times it appears in the dataset.\n",
        "* Useful for nominal categorical features, especially when one-hot encoding would create too many columns.\n",
        "\n",
        "Advantage:==>\n",
        "1. reduces dimensionality (single column).\n",
        "2. Preserves information about category popularity.\n",
        "\n",
        "Disadvantage:==>\n",
        "1. Doesnt capture relationship with target variable.\n",
        "2. May mislead models if frequency doesnt correlate with target.\n",
        "\n",
        "**6. Weight of Evidence (WOE) Encoding:**\n",
        "* Converts categories into numbers using the log of the odds ratio between positive and negative outcomes.\n",
        "* Best for binary classification problems (e.g., credit scoring).\n",
        "\n",
        "Advantages:==>\n",
        "1. Captures relationship between feature and target.\n",
        "2.  Works well with logistic regression.\n",
        "3. Helps detect predictive power.\n",
        "\n",
        "Disadvantages:==>\n",
        "- Only for binary targets.\n",
        "- Can cause data leakage if not used carefully.\n",
        "- Needs smoothing for rare categories.\n",
        "\n",
        "**7.Binary Encoding:**\n",
        "\n",
        "* Converts categorical values into binary digits (0s and 1s), then splits them into separate columns.\n",
        "* Best for high-cardinality categorical features where one-hot encoding would create too many columns.\n",
        "* Example :  Category A → 1 → 001 , Category B → 2 → 010, Category C → 3 → 011\n",
        "\n",
        " Advantage:==>*\n",
        "1. Reduces dimensionality compared to one-hot encoding.\n",
        "2. Efficient for large datasets with many categories.\n",
        "3. Preserves uniqueness without implying order.\n",
        "\n",
        "Disadvatage:==>\n",
        "1. Less interpretable than one-hot or label encoding.\n",
        "2.  May still introduce false relationships if not handled carefully.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "51_3VSe5ArKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What do you mean by training and testing a dataset?\n",
        "\n",
        "Answer :\n",
        "\n",
        "**1. Training Dataset:**\n",
        "\n",
        "-  It is the portion of data used to teach or train the model. The model learns patterns and relationships between the input features and output labels from this data.\n",
        "\n",
        "- The model adjusts its internal parameters (like weights) to minimize the error between its predictions and the actual targets.\n",
        "\n",
        "-\n",
        "1. supervised learning:  training data includes both input features and known output labels.\n",
        "\n",
        "2. In unsupervised learning: the training data is unlabeled, and the algorithm tries to find patterns or groupings (like clusters) by itself.\n",
        "\n",
        "**2. Testing dataset:**\n",
        "\n",
        "- It is the unseen data used to evaluate the model's performance after training. It checks how well the model generalizes to new, unseen data.\n",
        "\n",
        "- The testing dataset acts like an exam for the model—it checks how accurately the model predicts outcomes for data it hasn't seen before.\n",
        "\n",
        "- The testing phase helps detect problems like overfitting, where a model performs very well on training data but poorly on new data.\n",
        "\n",
        "3. In generaly data split to 80% training and 20% testing.\n",
        "\n",
        "4. The goal of splitting data is to avoid overfitting—when a model performs very well on training data but poorly on unseen data.\n",
        "\n",
        "5. we use sometime validation set used during training to fine-tune hyperparameters before the final evaluation on the test set.\n",
        "\n",
        "6. Example :   When we train a model to predict house prices, we use 80% of the data to teach it how features like area, location, and number of rooms affect price. The remaining 20% is used to test if it can correctly predict prices for houses it hasn't seen before"
      ],
      "metadata": {
        "id": "W8PJ-Z1LAt_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. What is sklearn.preprocessing?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "- In scikit-learn, sklearn.preprocessing is a module used for data preprocessing\n",
        "\n",
        "- preprocessing means getting your data ready before you train the model.\n",
        "\n",
        "- Machine learning algorithms work better when all inputs are in a consistent format and range.\n",
        "\n",
        "- The module has functions that automate this preparation process\n",
        "\n",
        "- transformers that prepare data before applying machine learning algorithms. It helps ensure that all features contribute proportionately to model learning and that the dataset's structure supports algorithm performance.\n",
        "\n",
        "- Example :     \n",
        "\n",
        "    1. MinMaxScaler : Scales features to a fixed range.  \n",
        "\n",
        "    2. StandardScaler: Standardizes features by removing the mean and scaling to unit variance.  \n",
        "\n",
        "    3. OneHotEncoder : Converts categorical variables into binary columns.                \n",
        "\n",
        "    4. Lable Encoder : Converts categorical labels into numeric values.\n",
        "\n",
        "    5. Imputer :  Fills missing values in the dataset.\n",
        "\n",
        "- preprocessor has two main step :           \n",
        "\n",
        "1. .fit() :  learn parameters (like mean, min, max)\n",
        "2. .transform() : to apply the transformation\n",
        "3. .fit_transform() use for training data and .transform() use for testing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mywf0O2CAz3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "kJsbAoRwJZ3m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. What is a Test set?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "A Test set in machine learning is a separate part of dataset that is kept completely independent[unseen] from the training process and used only at the very end to evaluate the final model's performance. Its main purpose is to provide an unbiased assessment of how well the trained model generalizes to new, unseen data—data that the model has never encountered before during training or tuning.\n",
        "\n",
        "1. It is usually 10% to 30% of the total dataset, depending on the project.\n",
        "2. The test set helps to detect overfitting, where the model might perform well on training data but poorly on new data.\n",
        "3. The distribution of the test set should be representative of the overall data distribution to ensure valid evaluation results.\n",
        "4. test set like final exam for the model, give realistic performance metrics like accuracy, precision, and recall.\n",
        "5. he model must never learn by the test set during the development process.\n",
        "6. Example :  If building a spam detection model, after training and validating it on most of your email data, you evaluate it on the test set of completely new emails. The test set assessment shows how well the model will perform in real-world email classification."
      ],
      "metadata": {
        "id": "nxYWwHfjA82w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. How do we split data for model fitting (training and testing) in Python?\n",
        "## How do you approach a Machine Learning problem?\n",
        "\n",
        "Answer :         \n",
        "\n",
        "**How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "Answer :  \n",
        "\n",
        "split data using **from sklearn.model_selection import train_test_split**\n",
        "\n",
        "In generally we split data for 80% training and 20% testing.\n",
        "\n",
        "Training set: Used to teach the model patterns in the data.\n",
        "\n",
        "Testing set: Used to evaluate how well the model performs on unseen data.\n",
        "\n",
        "**How do you approach a Machine Learning problem?**\n",
        "\n",
        "Answer :                   \n",
        "\n",
        "1. Understand the problem: Clarify objectives and expected output.\n",
        "\n",
        "2. Collect and explore data: Analyze data quality, missing values, and distributions.\n",
        "\n",
        "3. Preprocess data: Handle missing values, encode categorical variables, and scale features.[make it suitable for modeling.]\n",
        "\n",
        "4. Split the data: Divide into training and testing sets (and validation if needed).[to ensure unbiased evaluation]\n",
        "\n",
        "5. Choose a model: Pick an algorithm suitable for the problem type.\n",
        "\n",
        "6. Train the model: Fit the model using the training data.\n",
        "\n",
        "7. Evaluate the model: Use the test data to assess performance using appropriate metrics. [check model performance]\n",
        "\n",
        "8. Tune hyperparameters: Optimize model parameters to improve performance.[using for improve accuracy]\n",
        "\n",
        "8. Deploy and monitor: Implement the final model and monitor its performance over time.\n"
      ],
      "metadata": {
        "id": "0krVNorsBEMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X = features, y = labels\n",
        "# dataset = ...  # your data\n",
        "# X = dataset.drop('target', axis=1)\n",
        "# y = dataset['target']\n",
        "\n",
        "# Split data: 80% train, 20% test\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "4DKSjbspOLPM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps you understand your data thoroughly and prepare it effectively for modeling.\n",
        "\n",
        "1. Data understanding: EDA reveals the structure, types, and distribution of the data features, which is essential for choosing the right model and preprocessing techniques.\n",
        "\n",
        "2. Detecting problems: It helps identify missing values, errors, or outliers that could mislead the model during training and cause poor performance.\n",
        "\n",
        "3. Relationship insights: EDA uncovers correlations and relationships between variables, guiding feature selection and engineering for improved model accuracy.\n",
        "\n",
        "4. Assumption checking: It tests assumptions about the data that underlie modeling techniques, ensuring the chosen methods are appropriate.\n",
        "\n",
        "5. Informed decision making: EDA guides data cleaning, transformation, and variable selection for a robust, well-performing model.\n",
        "\n",
        "6. Avoids surprises: By exploring data patterns early, it prevents incorrect interpretations and overfitting caused by anomalies or flawed data.\n",
        "\n",
        "- EDA is an essential step that reduces risk, improves model quality, and makes data-driven insights more reliable before building a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "rgXLKTomBLaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. What is correlation?\n",
        "\n",
        "Answer :    \n",
        "\n",
        "1. Correlation is a statistical measure that describes the relationship between two variables, quantifying both the direction and strength of their association.\n",
        "\n",
        "2. Correlation measures how two variables move together. It shows if an increase in one variable corresponds to an increase or decrease in another.\n",
        "\n",
        "3. types :           \n",
        "\n",
        "- Positive correlation: Both variables increase or decrease together (e.g., hours studied and exam scores).\n",
        "\n",
        "- Negative correlation: One variable increases while the other decreases (e.g., hours watching TV and physical fitness).\n",
        "\n",
        "- No correlation: No consistent relationship exists between the variables.\n",
        "\n",
        "4. A numeric value ranging from -1 to +1 that indicates correlation strength and direction.[+1: perfect positive linear correlation,0: no linear correlation\n",
        ",-1: perfect negative linear correlation]\n",
        "\n",
        "5.\n",
        "- Correlation only measures linear relationships.\n",
        "\n",
        "- Sensitive to outliers.\n",
        "\n",
        "- Its important to remember that correlation does not mean one causes the other; it just shows a pattern of movement in opposite directions.\n",
        "\n",
        "6. In machine learning,Understanding correlations helps detect multicollinearity, select features, and understand data structure, which improves model building.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARSksZlrBan_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. What does negative correlation mean?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "Negative correlation means two variables move in opposite directions. When one increases, the other decreases. It is represented by a correlation coefficient close to -1.\n",
        "\n",
        "For example, as exercise increases, weight might decrease.\n",
        "\n",
        "In machine learning, understanding negative correlation helps identify inverse relationships between features, which is useful for model building.\n",
        "\n"
      ],
      "metadata": {
        "id": "ETpo_QyhBftu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. How can you find correlation between variables in Python?\n",
        "\n",
        "Answer :    \n",
        "\n"
      ],
      "metadata": {
        "id": "0x-WigqgBl75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pandas DataFrame .corr() method:\n",
        "# computes the pairwise correlation of columns in a DataFrame.\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4],\n",
        "    'B': [4, 3, 2, 1]\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "HSskd5frSM5U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kevpuUrqSTz6",
        "outputId": "21366cad-3160-4562-c6af-7bf695758796"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B\n",
            "A  1.0 -1.0\n",
            "B -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The correlation matrix shows that features A and B have a perfect negative correlation.\n",
        "# This means that as the values in feature A increase, the values in feature B decrease proportionally."
      ],
      "metadata": {
        "id": "67yUAnHNSnQa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using numpy corrcoef() function\n",
        "# calculates the Pearson correlation coefficient matrix for two numeric arrays.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array([4, 3, 2, 1])"
      ],
      "metadata": {
        "id": "fganAcMJSuai"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = np.corrcoef(x, y)\n",
        "print(corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BtDFwVKS8Cy",
        "outputId": "1d2fe5c8-7859-4acf-9fd4-bcee917def3a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. -1.]\n",
            " [-1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SciPy’s pearsonr() function:\n",
        "# Provides correlation coefficient and the p-value for testing non-correlation."
      ],
      "metadata": {
        "id": "1YMS1myrTHog"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "corr, p_value = pearsonr(x, y)\n",
        "print(corr, p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONCwE-WSTTFA",
        "outputId": "1b1a8000-38ab-42d0-d702-33288c9d5827"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Answer :\n",
        "\n",
        "- Causation means that a change in one variable directly causes a change in another variable. It implies a cause-and-effect relationship where one event or action leads to the occurrence of another.[Causation means one variable changes because of the other.]\n",
        "\n",
        "- Always Remember Correlation does not imply causation. To prove causation requires careful study and controlling for other factors.\n",
        "\n",
        "- Example : Heavy rainfall causes river water levels to rise, which may lead to flooding. Here, rainfall directly causes the water level to increase.\n",
        "\n",
        "| Aspect            | Correlation                                 | Causation                                    |\n",
        "|-------------------|---------------------------------------------|----------------------------------------------|\n",
        "| Meaning           | Shows a relationship or association between two variables | One variable directly causes the change in another |\n",
        "| Relationship type | Variables move together but no cause-effect implied | Cause and effect relationship exists         |\n",
        "| Example           | Ice cream sales and sunburn cases both increase in summer[However, buying ice cream does not cause sunburn.] | Smoking causes increased risk of lung cancer |\n",
        "| Does one cause the other? | No                                         | Yes                                          |\n",
        "| Interpretation    | Variables may both be influenced by a third factor or coincidence | One variable changes because of the other     |\n",
        "| Proof required    | No, just statistical evidence                | Yes, requires experimental or strong evidence |\n",
        "\n"
      ],
      "metadata": {
        "id": "qiynjVfNBshO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Answer :  \n",
        "\n",
        "- An optimizer is an algorithm that helps a machine learning model learn by adjusting its weights to reduce errors (loss).\n",
        "\n",
        "- need of optimizer :        \n",
        "\n",
        "    * To find the best values of model parameters that minimize the loss function, improving prediction accuracy and convergence speed.\n",
        "\n",
        "Types of optimizers:  \n",
        "\n",
        "A. Gradient Descent:\n",
        "\n",
        "1. Updates weights using the gradient of the loss over the entire dataset; simple but slow for large data.\n",
        "2. Updates weights after calculating gradients on the entire dataset per epoch.3. If dataset has 1000 rows and 5 clmns and 6 coefficient , epoch = 50\n",
        "\n",
        "in loop 50 times ,for single cofficent we calculate 1000 derivate.and 6 coefficent we calcualate 6000 detrivative and 50 epochas we calculate 6000*50=300000 derivative calculate [ in cnn and rnn not use]\n",
        "\n",
        "here we load all train data at a time,so its computenionaly expensive/required more hardware\n",
        "\n",
        "new_weight = old_weight - learning_rate * gradient of loss/slope\n",
        "\n",
        "- Gradient descent updates parameter by moving opposite to the slope.\n",
        "\n",
        "- Learning rate controls the step size.\n",
        "\n",
        "- Repeated updates bring you closer to the minimum.\n",
        "\n",
        "- Widely used to train machine learning models for optimal weights.\n",
        "\n",
        "3. Example : Training a simple linear regression model using all data points in each step.\n",
        "\n",
        "B. Stochastic Gradient Descent (SGD):\n",
        "\n",
        "1. Updates weights using gradients from one data point at a time; faster but noisy.\n",
        "\n",
        "2. Updates model parameters after each training example (one row at a time).\n",
        "\n",
        "For 1000 rows and 100 epochs, updates happen 100,000 times (one per row per epoch).\n",
        "\n",
        "- faster Covergen than gradient descent.\n",
        "- here we select row randomly\n",
        "- it's not give steady solution.\n",
        "- in big dataset stochastic doesn't required more epochs.\n",
        "- in non convex function/ when you have multiple local minima and one global minima always think about sgd[because rondomness/flcution not stuck in local minima]\n",
        "\n",
        "Cons :        \n",
        "1. Noisy Updates: Since updates are based on single samples, the gradient estimates are noisy and fluctuate a lot.\n",
        "2. Convergence Challenges: Because of noise, SGD can oscillate near minima and might not settle perfectly.\n",
        "3. Sensitive to Learning Rate: Requires careful tuning of learning rate; too high causes divergence, too low slows training.[ always use learning rate schedular]\n",
        "4. Requires Data Shuffling: To ensure randomness, data should be shuffled each epoch to avoid cycles or bias.\n",
        "\n",
        "3. Example of use case : Large datasets, online learning, Training a logistic regression model on a binary classification task,Training a neural network by updating weights after each sample or small mini-batch.\n",
        "\n",
        "\n",
        "C. Mini-batch Gradient Descent:\n",
        "\n",
        "1. Updates weights using a small batch of data at once, balancing speed and stability.\n",
        "\n",
        "2. The training dataset is divided into small groups called mini-batches (e.g., 32, 64, or 128 rows).\n",
        "\n",
        "3. The model's parameters are updated after computing gradients over each mini-batch rather than the entire dataset or a single data point.\n",
        "\n",
        "4. This method brings a balance between the stability of batch gradient descent and the speed of stochastic gradient descent.\n",
        "\n",
        "5. do batches update per epochs.[example 1000 row and 10 batch you take 10 update in one epoch]\n",
        "\n",
        "6. working:\n",
        "* Split dataset into mini-batches of fixed size.\n",
        "\n",
        "* For each mini-batch, calculate the average loss and gradients.\n",
        "\n",
        "* Update model parameters based on those gradients.\n",
        "\n",
        "* Repeat for all mini-batches (one epoch), and continue for multiple epochs.\n",
        "\n",
        "7. you can taking advantage tricking paramter like batch_size=n behave like gradient descet and batch_size=1 behave like sgd.\n",
        "\n",
        "8. Mini-batch gradient descent balances speed and accuracy by processing small batches of data for frequent but stable updates. It's the most commonly used optimizer variant in deep learning training.\n",
        "\n",
        "Advantage :                    \n",
        "\n",
        "- Computationally efficient: Uses vectorized operations, suitable for hardware acceleration (GPUs, TPUs).\n",
        "\n",
        "- Faster convergence: More frequent updates than batch GD without being as noisy as SGD.\n",
        "\n",
        "- Smoother convergence: Less noise compared to SGD, which leads to stable training.\n",
        "\n",
        "- Memory efficient: Does not require loading the entire dataset into memory at once.\n",
        "\n",
        "- Helps escape local minima by introducing slight randomness during updates.\n",
        "\n",
        "9. Example :  Neural network training with batches of size 32 or 64.\n",
        "\n",
        "D. Momentum :\n",
        "\n",
        "Momentum is an optimization technique that improves gradient descent by using the past velocity (update history) to smooth and accelerate learning.\n",
        "Instead of relying only on the current gradient, momentum accumulates an exponentially decaying average of past gradients — this helps the optimizer move faster, reduce oscillations, and escape local minima.\n",
        "\n",
        "1. mometumn solve problem of:\n",
        "\n",
        "      - high curvature\n",
        "      - consistent gradient - slope change very slowely here normal normal gradient descent algorithm stuck and slow because of update based on slope since slope is not change so algorithm become slow\n",
        "      - noisy gradient - beacuse of fluction can be stuck in local minima and you get suboptimum solution\n",
        "2. here we use previous velocity/histroy of update to build momemtum\n",
        "3. faster than mini batch and sgd.\n",
        "4. mometum take exponetially decay average of past gradients\n",
        "5. formula :  \n",
        "\n",
        "w_new = w_old - v_t\n",
        "\n",
        "v_t = beta * v_t-1 + learning_rate * derviative of current gradient at time t [0 <beta <1]\n",
        "\n",
        "v_t-1 = velocity at the previous steps\n",
        "\n",
        "beta = hyparameter\n",
        "\n",
        "beta = 0 means sgd\n",
        "\n",
        "beta means exponetially decay factor :  old velocity less impact , current velocity more impact  [ 1/1-beta]\n",
        "\n",
        "beta = 1 means there will be no decay and our algorithm oscilate on error\n",
        "\n",
        "surface continously oscilate because of no friction involve ,dynamic equlibrium.\n",
        "\n",
        "6. Working :         \n",
        "- Momentum behaves like physics-based inertia: the optimizer keeps moving in the direction it has been moving before.\n",
        "\n",
        "- Old velocity has exponentially less impact than the newer one → gives smoother and controlled acceleration.\n",
        "\n",
        "- It allows faster movement across flat surfaces and prevents the optimizer from getting stuck in local minima.\n",
        "\n",
        "pors :            \n",
        "\n",
        "- Speeds up convergence in training (faster learning).\n",
        "\n",
        "- Escapes local minima using accumulated direction.\n",
        "\n",
        "- Reduces oscillations in steep or noisy regions\n",
        "\n",
        "Cons :      \n",
        "- Needs careful tuning of hyperparameters.\n",
        "\n",
        "- Too high momentum causes overshoot and oscillations.\n",
        "\n",
        "7. Example : Used in CNNs for image classification\n",
        "\n",
        "\n",
        "E. Nag [Nesterov Accelerated Gradien] :\n",
        "\n",
        "Nesterov Accelerated Gradient (NAG) improves upon traditional momentum by computing the gradient not at the current position, but at a look-ahead position (where the momentum is about to take the parameters).\n",
        "This look-ahead step allows for more accurate and faster updates.\n",
        "\n",
        "- formula :\n",
        "\n",
        "w_look_ahead = w_t - beta * V_t-1\n",
        "\n",
        "v_t = beta * V_t-1 + learning_rate * gradient of look_ahead_point\n",
        "\n",
        "w_t+1 = w_t - V_t\n",
        "\n",
        "in nag we first calculate look ahead point using  mometum and then calculate current gradinet\n",
        "\n",
        "pros :                 \n",
        "- Faster Convergence: Speeds up training by anticipating the future position and reducing unnecessary oscillations.\n",
        "\n",
        "- More Accurate Updates: Computing gradients at the predicted future position avoids overshooting.\n",
        "\n",
        "- Better Stability: Less oscillation and smoother updates than classical momentum.\n",
        "\n",
        "- Better Generalization: Often results in models that generalize better to unseen data.\n",
        "\n",
        "- Requires Less Tuning: Fewer hyperparameters to adjust compared to some other optimizers.\n",
        "\n",
        "cons :         \n",
        "- beacause dam in oscillation you can stuck in local minima.\n",
        "- needs Good Hyperparameter Tuning\n",
        "- May Slow Down if Look-Ahead is Incorrect: If the look-ahead step is not accurate, updates can be less effective.\n",
        "     \n",
        "F. Adagrad:\n",
        "\n",
        "1. Adagrad is an optimization algorithm that adapts the learning rate for each parameter individually during training. It adjusts learning rates based on the historical squared gradients, allowing frequent parameters to have smaller learning rates and infrequent parameters to have larger learning rates.\n",
        "\n",
        "2. useful for sparse data and features that appear infrequently, like text or recommender systems.\n",
        "\n",
        "useful for input feature have different scale [example : age ,salary profit]\n",
        "\n",
        "3. here we doesn't fix learning rate, change learning rate based on scenario.\n",
        "\n",
        "4.working:\n",
        "- Keeps a cumulative sum of squared gradients for each parameter.\n",
        "\n",
        "- The learning rate for each parameter is scaled inversely proportional to the square root of this sum.\n",
        "\n",
        "- This means parameters with large gradients get smaller updates, stabilizing training.\n",
        "\n",
        "5. formula :          \n",
        "\n",
        "w_previous = w_current - learning rate * [ gradient of w_current / sqrt(v_t + small_number)]\n",
        "\n",
        "small_number = when v_t is 0 this term sqrt( + small_number)] not become 0.\n",
        "v_t = past_vt + (gradient)^2 ==> here you are storing sum of past [gradient]^2\n",
        "\n",
        "6. Example : Text classification using sparse word embeddings\n",
        "\n",
        "* Adagrad dynamically tunes learning rates per parameter based on gradient history, excelling in sparse settings but potentially slowing down learning over time if not managed.\n",
        "\n",
        "* Adgrad work very well on convex problem like linear regression.\n",
        "\n",
        "Pros:\n",
        "\n",
        "- Adapts learning rates automatically.\n",
        "\n",
        "- Works well for sparse data and features with varying importance.\n",
        "\n",
        "- Requires no manual tuning of learning rate for each parameter.\n",
        "\n",
        "Cons:\n",
        "\n",
        "- The accumulation of squared gradients can make the learning rate shrink too much, causing the training to stop improving (slow convergence).\n",
        "\n",
        "- This limitation is addressed by optimizers like RMSProp and Adam.\n",
        "\n",
        "- adgrad reach near to solution but doesn't converge global minima , take how many epochs beacuse of you are decreassing learning rate using past update/gradinet,as epochs increasing v_t incresing so lr decreases that's update become small , then stop  [not use in complex neural network]\n",
        "\n",
        "G. Rmsprop :   \n",
        "\n",
        "1. In adgrad we decrease lr , in one time learning rate become small and stuck in one place and doesn't converge,updation not happen, so overcome this problem we use rmsprop.\n",
        "\n",
        "2. formula :        \n",
        "\n",
        "v_t = beta * v_t-1 + (1-beta) * gradient of weight or bias [beta ideally 0.95]\n",
        "\n",
        "w_previous = w_current - learning rate * [ gradient of w_current / sqrt(v_t + small_number)]\n",
        "\n",
        "in adgrad we take all histroy gradient , here we take exponetially decay average , in rmsprop we give more value of recent epochs gradient and doesn't give value old epochs gradient so v_t term not become large in compare with adagrad that's why leraning rate not become small , this term learning rate * [ gradient of w_current / sqrt(v_t + small_number)] not become small and you can take update .\n",
        "\n",
        "- best optimization technique to optimize neural network.\n",
        "\n",
        "pros :      \n",
        "\n",
        "- Fast and stable training with adaptive steps.\n",
        "- Handles non-stationary objectives well (like in RNNs and reinforcement learning).\n",
        "- Smooths the optimization path by dampening oscillations.\n",
        "- Requires fewer hyperparameters, making tuning easier.\n",
        "\n",
        "Cons :\n",
        "\n",
        "- Needs good choices for learning rate and decay rate to work well.\n",
        "- Does not include momentum explicitly, which can slow progress in some scenarios.\n",
        "- Computational overhead due to maintaining moving averages of squared gradients.\n",
        "- Can struggle with oscillations or getting stuck in local minima without momentum.\n",
        "\n",
        "3. Example : Training an LSTM for time series forecasting.\n",
        "\n",
        "H. Adam [Adaptivbe moment estimation]:\n",
        "\n",
        "1. Adam is an enhanced version of RMSProp with momentum.\n",
        "2. Momentum helps smooth gradients by taking moving averages of past gradients.\n",
        "3. RMSProp adapts learning rates by dividing by a moving average of squared gradients.\n",
        "4. Adam maintains two moving averages:\n",
        "\n",
        "- First moment: mean of gradients (momentum)\n",
        "\n",
        "- Second moment: mean of squared gradients (adaptive learning rate)\n",
        "\n",
        "5. fromula :    \n",
        "\n",
        "w_previous = w_current - learning rate * [ gradient of w_current / sqrt(v_t + small_number)] * m_t\n",
        "\n",
        "m_t = beta_1 * m_t-1 + (1-beta_1) * gradient of weight vector  \n",
        "v_t = beta_2 * v_t-1 + (1-beta_2) * [gradient of weight]^2\n",
        "\n",
        "bias_correction :            \n",
        "m_t_hat = m_t / (1-beta_1^t)\n",
        "v_t_hat = V_t / (1-beta_2^t)\n",
        "\n",
        "t is epoch number in first t=1 second t=2 ...\n",
        "\n",
        "ideally beta_1 =0.9 and beta_2=0.99  you can change it if you want\n",
        "\n",
        "here we need bias because of in starting m_t and b_t is 0, so changing/upsetting here doing bias correction.\n",
        "\n",
        "6. Updates parameters using these bias-corrected moments.\n",
        "\n",
        "7. In adam you not need to change learning rate it's automatically change.\n",
        "\n",
        "8. Requires less tuning, often works well with default hyperparameters.\n",
        "\n",
        "9. Widely used for deep learning due to robustness and speed of convergence.\n",
        "\n",
        "10. Helps with noisy or sparse gradients by adapting updates per parameter.\n",
        "\n",
        "11. Sometimes may converge to suboptimal minima in fine-tuning tasks.\n",
        "\n",
        "12. in deep learning always first choice is adam.\n",
        "\n",
        "13. Example :   Training  a deep neural network for image recognition.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0EPCrX2sB0SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17. What is sklearn.linear_model ?\n",
        "\n",
        "Answer :   \n",
        "\n",
        "1. It is a module in Scikit-learn library that contains various linear models for regression and classification.\n",
        "2. To build models that assume a linear relationship between input features and the target variable.\n",
        "3. Common models/algorithm :    \n",
        "- LinearRegression (for regression)\n",
        "\n",
        "- LogisticRegression (for classification)\n",
        "\n",
        "- Ridge, Lasso, ElasticNet (regularized regression)\n",
        "\n",
        "- SGDClassifier, Perceptron\n",
        "4. Usage :                        \n",
        "- Simple and efficient for linear relationships\n",
        "\n",
        "- Easy to interpret model coefficients\n",
        "\n",
        "- Can be used as a baseline for more complex models\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H8W4DKRGB55N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#model = LinearRegression()\n",
        "#model.fit(X_train, y_train)          # Train model\n",
        "#predictions = model.predict(X_test) # Predict on new data\n"
      ],
      "metadata": {
        "id": "v9RC_Bouentw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Answer :   \n",
        "\n",
        "- The fit() method is used to train the model on the provided dataset.\n",
        "- It learns the relationship between input features (X) and target labels (y) by finding the best parameters (e.g., coefficients in linear regression) that minimize errors.\n",
        "- After calling fit(), the model is ready to make predictions on new data.\n",
        "\n",
        "- Arguments:                       \n",
        "1. X(features) :   The input data, usually a 2D array or matrix where rows are samples and columns are features.\n",
        "2. y(Target) :  The target/output values or labels corresponding to each input sample in X. For regression, these are continuous values; for classification, class labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "2OI6nM9rCChd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = [[1], [2], [3], [4]]   # Features\n",
        "y = [2, 4, 6, 8]           # Target values\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)             # Train the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "YSIV6PE3fyMd",
        "outputId": "f2f6d662-9408-47b1-a265-dcb5ef1c400b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "- Makes predictions on new input data using the trained model.\n",
        "- After training with .fit(), use .predict() to find output values for unseen data.\n",
        "\n",
        "Argument:  \n",
        "1. Accepts input features (X) as a 2D array or matrix (samples x features).\n",
        "2. Returns predicted values (labels or continuous outputs).\n",
        "\n",
        "Example :  "
      ],
      "metadata": {
        "id": "_mSgMHCJCAdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = model.predict(X_test)\n",
        "# X_test is new/unseen data, and .predict() will be the model’s best guess for outputs."
      ],
      "metadata": {
        "id": "qIXmGLZHgeOs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20. What are continuous and categorical variables?\n",
        "\n",
        "Answer :  \n",
        "\n",
        "Continuous:   \n",
        "- numbers you measure (can be decimals).\n",
        "- Can take any value within a range.\n",
        "- Examples: Height, weight, temperature, time.\n",
        "- In regression based algorithm target column are continous.\n",
        "\n",
        "Categorical :                   \n",
        "- labels or groups you count.\n",
        "- Take specific categories\n",
        "- Example :  Color (red, blue), Gender (male, female), Type of fruit.\n",
        "- classification problem target variable are categorical/discrete.\n",
        "- Convert into use different different encoding techniques.\n",
        "- types :               \n",
        "1. nomial :- no order\n",
        "2. ordinal : order"
      ],
      "metadata": {
        "id": "8vV-OmiTCNxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Answer :\n",
        "\n",
        "Feature scaling is a crucial preprocessing step for better and faster machine learning models.\n",
        "\n",
        "**What is Feature Scaling?**\n",
        "\n",
        "1. Feature Scaling means changing the values of features so they are on the same scale or range.\n",
        "2. It makes important features equally weighted by bringing them to a common scale.\n",
        "3. It is done before training machine learning models.\n",
        "4. Types:    \n",
        "- Normalization: Scale values to a range (0 to 1).    \n",
        "\n",
        "    formula = (x - min)/(max_min)\n",
        "\n",
        "- Standardization: Scale data to have mean 0 and standard deviation 1.\n",
        "\n",
        "    formula = x - mu / sigma   =====> mu =0 and sigma=1\n",
        "\n",
        "**How does it help in Machine Learning?**\n",
        "1. Improves the accuracy and performance of many algorithms.\n",
        "2. Helps algorithms like regression,KNN, SVM, Gradient Descent work better by avoiding bias toward features with larger values.\n",
        "3. Makes training faster and more stable.\n",
        "4. Prevents features with large ranges from dominating others.\n",
        "\n",
        "\n",
        "**Example:**\n",
        "age : might be range in 23 and 58.\n",
        "\n",
        "salary : might be range in 35000 and 250000.\n",
        "\n",
        "- the difference in the salary is much bigger than age.\n",
        "\n",
        "- When machine learning models calculate distances or updates (like in gradient descent), the feature with the larger range (Salary) influences the result more.\n",
        "\n",
        "- This causes the model to pay more attention to Salary, ignoring Age even if Age is important.\n",
        "\n",
        "- It leads to slow or inaccurate learning where Age effect is underestimated.\n",
        "\n",
        "- Use feature scaling (like normalization or standardization) to put Age and Salary on the same scale.\n",
        "\n",
        "- This way, models treat both features equally and learn better.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L-nwJMNACTbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22. How do we perform scaling in Python?\n",
        "\n",
        "Answer :    "
      ],
      "metadata": {
        "id": "JqQdLMMFCZcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "dhNMsqqjkQTE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  columns are Age and Salary\n",
        "data = np.array([[25, 50000],\n",
        "                 [40, 120000],\n",
        "                 [35, 80000]])"
      ],
      "metadata": {
        "id": "jjEPW87HkScz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler() # create obj\n"
      ],
      "metadata": {
        "id": "JtAxKZ-4kTYD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data = scaler.fit_transform(data)  # Fit and transform data"
      ],
      "metadata": {
        "id": "nXGXQnX4kZjS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUfwLO4xkdGq",
        "outputId": "e1bedfc1-b831-4865-d3f9-8335218e7be8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.        ]\n",
            " [1.         1.        ]\n",
            " [0.66666667 0.42857143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 23. What is sklearn.preprocessing?\n",
        "\n",
        "Answer :   \n",
        "\n",
        "- sklearn.preprocessing is a module in Scikit-learn for data preprocessing.\n",
        "- It helps prepare your data before feeding it into machine learning models.\n",
        "-\n",
        "1. Scaling features (e.g., StandardScaler, MinMaxScaler)\n",
        "\n",
        "2. Encoding categorical variables (e.g., OneHotEncoder, LabelEncoder)\n",
        "\n",
        "3. Normalizing, binarizing, imputing missing values,etc.\n",
        "\n",
        "- Makes data clean, consistent, and ready for better model performance.\n",
        "\n",
        "- example:"
      ],
      "metadata": {
        "id": "2vFmfw17CfG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eCbMrCTSlB5a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    'age': [25, 40, 35],\n",
        "    'salary': [50000, 120000, 80000],\n",
        "    'gender': ['M', 'F', 'M']\n",
        "})\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjb-8DX9lFdJ",
        "outputId": "374f8d3a-fe42-4010-bba5-eb6ff75efd53"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  salary gender\n",
            "0   25   50000      M\n",
            "1   40  120000      F\n",
            "2   35   80000      M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numeric columns\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "UzUB4mP1lJLi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['age', 'salary']] = scaler.fit_transform(data[['age', 'salary']])"
      ],
      "metadata": {
        "id": "ul9k2ssElNpx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical column\n",
        "encoder = OneHotEncoder(sparse_output=False)"
      ],
      "metadata": {
        "id": "1T54aH1vlQ5B"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_gender = encoder.fit_transform(data[['gender']])"
      ],
      "metadata": {
        "id": "xfxy-p-flVLJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_gender_df = pd.DataFrame(encoded_gender, columns=encoder.get_feature_names_out(['gender']))"
      ],
      "metadata": {
        "id": "RbjOTi9PlYn1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine\n",
        "data = pd.concat([data, encoded_gender_df], axis=1).drop('gender', axis=1)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9MnJ0egla15",
        "outputId": "2a87ecbd-fa3b-4067-b5f1-67dd64ca048a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        age    salary  gender_F  gender_M\n",
            "0 -1.336306 -1.162476       0.0       1.0\n",
            "1  1.069045  1.278724       1.0       0.0\n",
            "2  0.267261 -0.116248       0.0       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Answer :\n",
        "\n",
        "- Randomly divides dataset into training and testing subsets.\n",
        "- Training data (X_train, y_train) is used to fit the model.\n",
        "- Testing data (X_test, y_test) is used to evaluate model performance on unseen data.\n",
        "- This method ensures your model is tested fairly on new data, preventing overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "iQfuWZlACj0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split function from Scikit-learn's model_selection module.\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "RiD5Xr3UmGDS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare your features (X) and target (y).\n",
        "#Split the data:\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "#test_size=0.20 means 20% data for testing, 80% for training.\n",
        "#random_state=42 makes the split reproducible.\n"
      ],
      "metadata": {
        "id": "YEIlodCvmQ0m"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25. Explain data encoding?\n",
        "\n",
        "Answer :\n",
        "\n",
        "Data encoding is crucial to convert real-world categorical data into a machine-readable format for better model performance.\n",
        "\n",
        "- Data encoding is the process of converting categorical or text data into numbers so that machine learning models can understand and use them.\n",
        "\n",
        "- Machine learning models cannot work directly with text like red,blue,small,large [Most ML algorithms work with numbers, not words or labels.]\n",
        "\n",
        "- Encoding transforms these into numerical representations like 0, 1, 2, or binary vectors.\n",
        "\n",
        "- Helps models interpret categories correctly and learn meaningful patterns.\n",
        "\n",
        "- Techniques:    \n",
        "\n",
        "1. One-Hot Encoding: Creates separate binary columns for each category (e.g red,blue)\n",
        "\n",
        "2. Label Encoding: Assigns a unique integer to each category (e.g. red → 0, blue → 1).\n",
        "\n",
        "3. Ordinal Encoding: Like label encoding but preserves order in categories (e.g. small → 0, medium →1, large → 2)."
      ],
      "metadata": {
        "id": "4kUtxrJ4Cqpr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_L1_Ly2Anip"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}